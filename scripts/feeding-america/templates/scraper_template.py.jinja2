"""Scraper for {{ food_bank_name }}."""

import asyncio
import json
import logging
from typing import Any, Dict, List, Optional

import httpx
import requests
from bs4 import BeautifulSoup

from app.scraper.utils import ScraperJob, get_scraper_headers

logger = logging.getLogger(__name__)


class {{ class_name }}Scraper(ScraperJob):
    """Scraper for {{ food_bank_name }}."""

    def __init__(self, scraper_id: str = "{{ scraper_id }}", test_mode: bool = False) -> None:
        """Initialize scraper with ID '{{ scraper_id }}' by default.

        Args:
            scraper_id: Optional custom scraper ID, defaults to '{{ scraper_id }}'
            test_mode: If True, only process limited data for testing
        """
        super().__init__(scraper_id=scraper_id)
        
        # TODO: Update this URL based on the food bank's website
        self.url = "{{ food_bank_url }}"
        self.test_mode = test_mode
        
        # For API-based scrapers
        self.batch_size = 10 if not test_mode else 3
        self.request_delay = 0.5 if not test_mode else 0.05
        self.timeout = 30.0

    async def download_html(self) -> str:
        """Download HTML content from the website.

        Returns:
            str: Raw HTML content

        Raises:
            requests.RequestException: If download fails
        """
        logger.info(f"Downloading HTML from {self.url}")
        response = requests.get(self.url, headers=get_scraper_headers(), timeout=self.timeout)
        response.raise_for_status()
        return response.text

    async def fetch_api_data(self, endpoint: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Fetch data from API endpoint (for API-based scrapers).

        Args:
            endpoint: API endpoint path
            params: Optional query parameters

        Returns:
            API response as dictionary

        Raises:
            httpx.HTTPError: If API request fails
        """
        url = f"{self.url}/{endpoint}" if endpoint else self.url
        
        try:
            async with httpx.AsyncClient(
                headers=get_scraper_headers(), 
                timeout=httpx.Timeout(self.timeout, connect=self.timeout/3)
            ) as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
                return response.json()
        except httpx.HTTPError as e:
            logger.error(f"HTTP error fetching data from {url}: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error fetching data from {url}: {e}")
            raise

    def parse_html(self, html: str) -> List[Dict[str, Any]]:
        """Parse HTML to extract food pantry information.

        Args:
            html: Raw HTML content

        Returns:
            List of dictionaries containing food pantry information
        """
        soup = BeautifulSoup(html, "html.parser")
        locations: List[Dict[str, Any]] = []
        
        # TODO: Update selectors based on actual website structure
        # Common patterns to look for:
        # - Tables with location data
        # - Divs/sections with location cards
        # - Lists of locations
        
        # Example: Find location containers
        location_elements = soup.find_all('div', class_='location')  # Update selector
        
        for element in location_elements:
            # Extract information from each location
            location = {
                "name": "",  # TODO: Extract name
                "address": "",  # TODO: Extract address
                "city": "",  # TODO: Extract city
                "state": "{{ state }}",
                "zip": "",  # TODO: Extract zip
                "phone": "",  # TODO: Extract phone
                "hours": "",  # TODO: Extract hours
                "services": [],  # TODO: Extract services
                "website": "",  # TODO: Extract website
                "notes": "",  # TODO: Extract any additional notes
                # Lat/long is optional - validator service will geocode if missing
                "latitude": None,  # TODO: Extract if available
                "longitude": None,  # TODO: Extract if available
            }
            
            # Skip empty items
            if not location["name"]:
                continue
                
            locations.append(location)
        
        logger.info(f"Parsed {len(locations)} locations from HTML")
        return locations

    def process_api_response(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process API response data.

        Args:
            data: API response data

        Returns:
            List of dictionaries containing location information
        """
        locations: List[Dict[str, Any]] = []
        
        # TODO: Extract locations from API response
        # Common patterns:
        # - data['results'] or data['locations'] or data['items']
        # - May need to handle pagination
        
        # Example:
        # for item in data.get('locations', []):
        #     location = {
        #         "id": item.get("id"),
        #         "name": item.get("name", "").strip(),
        #         "address": item.get("address", "").strip(),
        #         "city": item.get("city", "").strip(),
        #         "state": item.get("state", "{{ state }}").strip(),
        #         "zip": item.get("zip", "").strip(),
        #         "phone": item.get("phone", "").strip(),
        #         "latitude": item.get("latitude"),
        #         "longitude": item.get("longitude"),
        #         "hours": item.get("hours", ""),
        #         "services": item.get("services", []),
        #     }
        #     locations.append(location)
        
        logger.info(f"Processed {len(locations)} locations from API")
        return locations

    async def scrape(self) -> str:
        """Scrape data from the source.

        Returns:
            Raw scraped content as JSON string
        """
        # TODO: Choose the appropriate scraping approach
        
        # Option 1: HTML Scraping
        # =====================
        # Download and parse HTML
        html = await self.download_html()
        locations = self.parse_html(html)
        
        # Option 2: API Scraping
        # =====================
        # # Fetch data from API
        # response = await self.fetch_api_data("endpoint/path", params={"key": "value"})
        # locations = self.process_api_response(response)
        
        # Option 3: Grid-based API Search (for APIs with geographic search)
        # ================================================================
        # from app.models.geographic import GridPoint
        # 
        # # Get grid points for the state
        # grid_points = self.utils.get_state_grid_points("{{ state|lower }}")
        # 
        # # Limit grid points in test mode
        # if self.test_mode:
        #     grid_points = grid_points[:3]
        # 
        # locations = []
        # for i, point in enumerate(grid_points):
        #     if i > 0:
        #         await asyncio.sleep(self.request_delay)
        #     
        #     # Search around this grid point
        #     response = await self.fetch_api_data(
        #         "search", 
        #         params={"lat": point.lat, "lng": point.lng, "radius": 50}
        #     )
        #     locations.extend(self.process_api_response(response))
        
        # Deduplicate locations if needed
        unique_locations = []
        seen_ids = set()
        
        for location in locations:
            # Create unique ID (adjust based on your data)
            location_id = f"{location.get('name', '')}_{location.get('address', '')}"
            
            if location_id not in seen_ids:
                seen_ids.add(location_id)
                unique_locations.append(location)
        
        logger.info(f"Found {len(unique_locations)} unique locations (from {len(locations)} total)")
        
        # Process each location
        job_count = 0
        
        for location in unique_locations:
            # Note: Geocoding is now handled by the validator service
            # Scrapers should provide lat/long if available, otherwise leave as None
            
            # Add metadata
            location["source"] = "{{ scraper_id }}"
            location["food_bank"] = "{{ food_bank_name }}"
            
            # Submit to queue
            job_id = self.submit_to_queue(json.dumps(location))
            job_count += 1
            logger.debug(f"Queued job {job_id} for location: {location.get('name', 'Unknown')}")
        
        # Create summary
        summary = {
            "scraper_id": self.scraper_id,
            "food_bank": "{{ food_bank_name }}",
            "total_locations_found": len(locations),
            "unique_locations": len(unique_locations),
            "total_jobs_created": job_count,
            "source": self.url,
            "test_mode": self.test_mode
        }
        
        # Print summary to CLI
        print(f"\n{'='*60}")
        print(f"SCRAPER SUMMARY: {{ food_bank_name }}")
        print(f"{'='*60}")
        print(f"Source: {self.url}")
        print(f"Total locations found: {len(locations)}")
        print(f"Unique locations: {len(unique_locations)}")
        print(f"Jobs created: {job_count}")
        if self.test_mode:
            print(f"TEST MODE: Limited processing")
        print(f"Status: Complete")
        print(f"{'='*60}\n")
        
        # Return summary for archiving
        return json.dumps(summary)